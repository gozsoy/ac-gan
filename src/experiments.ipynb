{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# roadmap:\n",
    "# 1-implement dcgan in tf and observe generated examples\n",
    "# 2-investigate tf dcgan implementation\n",
    "\n",
    "# 3-implement dcgan in torch and observed generated examples\n",
    "# 4-investigate torch dcgan implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 23:32:33.714503: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# [0,255] -> [0,1] -> [-1,1]\n",
    "x_train = (x_train/255.) * 2. - 1.\n",
    "\n",
    "x_train = np.expand_dims(x_train,axis=3)\n",
    "x_train = tf.cast(x_train,dtype=tf.float32)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_ds = train_ds.shuffle(1000).batch(64)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# GENERATOR\n",
    "Generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(100,)),\n",
    "        layers.Dense(7 * 7 * 100),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((7, 7, 100)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"tanh\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "\n",
    "# DISCRIMINATOR\n",
    "Discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(64, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True,label_smoothing=0.0)\n",
    "\n",
    "# optimizers\n",
    "gen_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0002)\n",
    "\n",
    "# metrics\n",
    "disc_loss_tracker = tf.keras.metrics.Mean(name='disc_loss')\n",
    "gen_loss_tracker = tf.keras.metrics.Mean(name='gen_loss')\n",
    "\n",
    "# tensorboard\n",
    "experiment_name = 'deneme'\n",
    "log_dir = '../logs/'+experiment_name\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "latent_code_size = 100\n",
    "# fix latent code to track improvement\n",
    "latent_code4visualization = tf.random.normal(shape=(16,latent_code_size))\n",
    "epochs = 50\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for _,real_imgs in train_ds.enumerate():\n",
    "        \n",
    "        # PART 1: DISC TRAINING, fixed generator\n",
    "        latent_code = tf.random.normal(shape=(real_imgs.shape[0],latent_code_size))\n",
    "\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            # generate fake images\n",
    "            generated_imgs = Generator(latent_code,training=True)\n",
    "\n",
    "            # forward pass real and fake images\n",
    "            real_preds,fake_preds = Discriminator(real_imgs),Discriminator(generated_imgs)\n",
    "            y_pred = tf.concat([real_preds,fake_preds],axis=0)\n",
    "            y_true = tf.concat([tf.ones_like(real_preds),tf.zeros_like(fake_preds)],axis=0)\n",
    "            \n",
    "            # compute loss\n",
    "            disc_loss = loss_fn(y_true=y_true,y_pred=y_pred)\n",
    "\n",
    "        # compute disc gradients\n",
    "        disc_gradients = disc_tape.gradient(disc_loss,Discriminator.variables)\n",
    "\n",
    "        # update disc weights\n",
    "        disc_optimizer.apply_gradients(zip(disc_gradients, Discriminator.variables))\n",
    "\n",
    "        # update disc metrics\n",
    "        disc_loss_tracker.update_state(disc_loss)\n",
    "\n",
    "\n",
    "        # PART 2: GEN TRAINING, fixed discriminator\n",
    "        latent_code = tf.random.normal(shape=(real_imgs.shape[0],latent_code_size))\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            # generate fake images\n",
    "            generated_imgs = Generator(latent_code,training=True)\n",
    "\n",
    "            # forward pass only images\n",
    "            fake_preds = Discriminator(generated_imgs)\n",
    "\n",
    "            # compute loss\n",
    "            gen_loss = loss_fn(y_true=tf.ones_like(fake_preds),y_pred=fake_preds)\n",
    "\n",
    "        # compute gen gradients\n",
    "        gen_gradients = gen_tape.gradient(gen_loss,Generator.variables)\n",
    "\n",
    "        # update gen weights\n",
    "        gen_optimizer.apply_gradients(zip(gen_gradients, Generator.variables))\n",
    "\n",
    "        # update gen metrics\n",
    "        gen_loss_tracker.update_state(gen_loss)\n",
    "\n",
    "\n",
    "    # generate sample images per epoch\n",
    "    test_generated_imgs = Generator(latent_code4visualization,training=False)\n",
    "    test_generated_imgs = (test_generated_imgs+1.)/2.\n",
    "    \n",
    "    # display and record metrics at the end of each epoch.\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('disc_loss', disc_loss_tracker.result(), step=epoch)\n",
    "        tf.summary.scalar('gen_loss', gen_loss_tracker.result(), step=epoch)\n",
    "        tf.summary.image(name='test_samples',data=test_generated_imgs,max_outputs=test_generated_imgs.shape[0],step=epoch)\n",
    "\n",
    "    disc_loss,gen_loss = disc_loss_tracker.result(),gen_loss_tracker.result()\n",
    "    print(f'epoch: {epoch}, disc_loss: {disc_loss:.4f}, gen_loss: {gen_loss:.4f}')\n",
    "\n",
    "    # reset metric states\n",
    "    disc_loss_tracker.reset_state()\n",
    "    gen_loss_tracker.reset_state()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "17915d4eccf26051373144ab496c4cfde1d85bab0b3b06c6ac905c8927260055"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('ml4hc_project2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
