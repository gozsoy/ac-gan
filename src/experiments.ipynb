{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-30 23:32:33.714503: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# [0,255] -> [0,1] -> [-1,1]\n",
    "x_train = (x_train/255.) * 2. - 1.\n",
    "\n",
    "x_train = np.expand_dims(x_train,axis=3)\n",
    "x_train = tf.cast(x_train,dtype=tf.float32)\n",
    "\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "train_ds = train_ds.shuffle(1000).batch(256)\n",
    "train_ds = train_ds.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-19 11:01:16.646889: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# GENERATOR\n",
    "Generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(128,)),\n",
    "        layers.Dense(7 * 7 * 128),\n",
    "        layers.Reshape((7, 7, 128)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(1, (7, 7), padding=\"same\", activation=\"tanh\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "\n",
    "# DISCRIMINATOR\n",
    "Discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(28, 28, 1)),\n",
    "        layers.Conv2D(64, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_1 (Conv2D)           (None, 14, 14, 64)        1088      \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 14, 14, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 7, 7, 128)         131200    \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 4, 4, 128)         262272    \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " global_max_pooling2d (Globa  (None, 128)              0         \n",
      " lMaxPooling2D)                                                  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 129       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 394,689\n",
      "Trainable params: 394,689\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "Discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True,label_smoothing=0.1)\n",
    "\n",
    "# optimizers\n",
    "gen_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0004)\n",
    "disc_optimizer = tf.keras.optimizers.Adam(learning_rate = 0.0003)\n",
    "\n",
    "# metrics\n",
    "disc_loss_tracker = tf.keras.metrics.Mean(name='disc_loss')\n",
    "gen_loss_tracker = tf.keras.metrics.Mean(name='gen_loss')\n",
    "\n",
    "# tensorboard\n",
    "experiment_name = 'lbl01_glr0004_dlr0003_g13m_d390k'\n",
    "log_dir = '../logs/'+experiment_name\n",
    "img_save_dir = '../generated_imgs/'+experiment_name\n",
    "summary_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "latent_code_size = 128\n",
    "# fix latent code to track improvement\n",
    "latent_code4visualization = tf.random.normal(shape=(25,latent_code_size))\n",
    "epochs = 30\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    for _,real_imgs in train_ds.enumerate():\n",
    "        \n",
    "        # PART 1: DISC TRAINING, fixed generator\n",
    "        latent_code = tf.random.normal(shape=(real_imgs.shape[0],latent_code_size))\n",
    "\n",
    "        with tf.GradientTape() as disc_tape:\n",
    "            # generate fake images\n",
    "            generated_imgs = Generator(latent_code)\n",
    "\n",
    "            # forward pass real and fake images\n",
    "            real_preds,fake_preds = Discriminator(real_imgs),Discriminator(generated_imgs)\n",
    "            y_pred = tf.concat([real_preds,fake_preds],axis=0)\n",
    "            y_true = tf.concat([tf.ones_like(real_preds),tf.zeros_like(fake_preds)],axis=0)\n",
    "            \n",
    "            # compute loss\n",
    "            disc_loss = loss_fn(y_true=y_true,y_pred=y_pred)\n",
    "\n",
    "        # compute disc gradients\n",
    "        disc_gradients = disc_tape.gradient(disc_loss,Discriminator.trainable_variables)\n",
    "\n",
    "        # update disc weights\n",
    "        disc_optimizer.apply_gradients(zip(disc_gradients, Discriminator.trainable_variables))\n",
    "\n",
    "        # update disc metrics\n",
    "        disc_loss_tracker.update_state(disc_loss)\n",
    "\n",
    "\n",
    "        # PART 2: GEN TRAINING, fixed discriminator\n",
    "        latent_code = tf.random.normal(shape=(real_imgs.shape[0],latent_code_size))\n",
    "\n",
    "        with tf.GradientTape() as gen_tape:\n",
    "            # generate fake images\n",
    "            generated_imgs = Generator(latent_code)\n",
    "\n",
    "            # forward pass only images\n",
    "            fake_preds = Discriminator(generated_imgs)\n",
    "\n",
    "            # compute loss\n",
    "            gen_loss = loss_fn(y_true=tf.ones_like(fake_preds),y_pred=fake_preds)\n",
    "\n",
    "        # compute gen gradients\n",
    "        gen_gradients = gen_tape.gradient(gen_loss,Generator.trainable_variables)\n",
    "\n",
    "        # update gen weights\n",
    "        gen_optimizer.apply_gradients(zip(gen_gradients, Generator.trainable_variables))\n",
    "\n",
    "        # update gen metrics\n",
    "        gen_loss_tracker.update_state(gen_loss)\n",
    "\n",
    "\n",
    "    # generate and save sample images per epoch\n",
    "    test_generated_imgs = Generator(latent_code4visualization)\n",
    "    test_generated_imgs = (((test_generated_imgs+1.)/2.) * 255.).numpy()\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "    for i in range(test_generated_imgs.shape[0]):\n",
    "        plt.subplot(5, 5, i+1)\n",
    "        plt.imshow(test_generated_imgs[i,:,:,0], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.savefig(img_save_dir)\n",
    "    \n",
    "\n",
    "    # display and record metrics at the end of each epoch.\n",
    "    with summary_writer.as_default():\n",
    "        tf.summary.scalar('disc_loss', disc_loss_tracker.result(), step=epoch)\n",
    "        tf.summary.scalar('gen_loss', gen_loss_tracker.result(), step=epoch)\n",
    "        tf.summary.image(name='test_samples',data=test_generated_imgs,max_outputs=test_generated_imgs.shape[0],step=epoch)\n",
    "\n",
    "    disc_loss,gen_loss = disc_loss_tracker.result(),gen_loss_tracker.result()\n",
    "    print(f'epoch: {epoch}, disc_loss: {disc_loss:.4f}, gen_loss: {gen_loss:.4f}')\n",
    "\n",
    "    # reset metric states\n",
    "    disc_loss_tracker.reset_state()\n",
    "    gen_loss_tracker.reset_state()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a73d1662bd3aab4de55a1a51be85519c6e25d5d617da76d142a49d5ef38ee143"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('torch_tf_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
